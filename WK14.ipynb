{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 14\n",
    "\n",
    "Convolutional Neural Network and Residual Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/src/data_utils.py\n",
    "!wget -q https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/src/image_utils.py\n",
    "!wget -q https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/src/nn_utils.py\n",
    "!wget -q https://github.com/PSAM-5020-2025S-A/WK14/raw/main/WK14_utils.py\n",
    "\n",
    "!wget -qO- https://github.com/PSAM-5020-2025S-A/5020-utils/releases/latest/download/lfw.tar.gz | tar xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from data_utils import LFWUtils, classification_error, display_confusion_matrix\n",
    "from image_utils import make_image\n",
    "from nn_utils import get_labels, get_num_params\n",
    "\n",
    "from WK14_utils import display_activation_grids, display_kernel_grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Review\n",
    "\n",
    "Let's continue exploring the _Labeled Faces in the Wild_ dataset from last week.\n",
    "\n",
    "We'll start with the multi-layer network we had in the end of class and add some normalization and dropout to help with training.\n",
    "\n",
    "Just as a review, the steps for setting up a classification (or regression) model are:\n",
    "\n",
    "- Load dataset and do any kind of pre-pre-processing\n",
    "- Split data into train/test datasets\n",
    "- Perform any kind of pre-processing\n",
    "- Split independent features and classification label and load them into `Tensors`\n",
    "- Build a NN model\n",
    "- Set up an optimizer\n",
    "- Pick a cost/loss function\n",
    "- Implement an evaluation function and any other kind of visualization that helps quantify/evaluate the model\n",
    "- Train model\n",
    "- Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and split Dataset\n",
    "\n",
    "The `LFWUtils.train_test_split(0.5)` function gives us some `Python` objects we can use to create our `Tensor`s.\n",
    "\n",
    "The `pixels` key gives us a list of the images' pixel data, and the `label` key gives us the images' label IDs.\n",
    "\n",
    "We don't have to do any normalization since the pixels will be in a known, well-defined, range of $[0 \\text{ - } 255]$.\n",
    "\n",
    "The only thing we have to do differently is cast the labels `Tensor` to `long`. This is to ensure the numbers in those `Tensor`s are whole numbers and don't have decimal points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = LFWUtils.train_test_split(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at data\n",
    "\n",
    "We can visualize some of the images, their text labels and label IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "id = train[\"labels\"][idx]\n",
    "print(id, LFWUtils.LABELS[id])\n",
    "display(make_image(train[\"pixels\"][idx], width=130))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron\n",
    "\n",
    "Let's create a neural network with multiple hidden layers.\n",
    "\n",
    "In order to keep the training manageable on a cpu, we'll use `PCA` to reduce the dimensions of our input samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = StandardScaler()\n",
    "pca = PCA(n_components=0.9999)\n",
    "\n",
    "x_pca_train = pca.fit_transform(std.fit_transform(train[\"pixels\"]))\n",
    "x_pca_test = pca.transform(std.transform(test[\"pixels\"]))\n",
    "\n",
    "print(pca.n_components_, pca.explained_variance_ratio_.sum())\n",
    "\n",
    "x_train = Tensor(x_pca_train)\n",
    "y_train = Tensor(train[\"labels\"]).long()\n",
    "\n",
    "x_test = Tensor(x_pca_test)\n",
    "y_test = Tensor(test[\"labels\"]).long()\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  nn.Sequential(\n",
    "  nn.Linear(x_train.shape[1], x_train.shape[1] // 2),\n",
    "  nn.Sigmoid(),\n",
    "\n",
    "  nn.Linear(x_train.shape[1] // 2, x_train.shape[1] // 4),\n",
    "  nn.Sigmoid(),\n",
    "\n",
    "  nn.Linear(x_train.shape[1] // 4, x_train.shape[1] // 16),\n",
    "  nn.Sigmoid(),\n",
    "\n",
    "  nn.Linear(x_train.shape[1] // 16, len(y_train.unique())),\n",
    ")\n",
    "\n",
    "learning_rate = 1e-1\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "out = model(x_train)\n",
    "\n",
    "print(\"Input shape:\", x_train.shape)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(\"Num Params:\", get_num_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(32):\n",
    "  optim.zero_grad()\n",
    "  labels_pred = model(x_train)\n",
    "  loss = loss_fn(labels_pred, y_train)\n",
    "  loss.backward()\n",
    "  optim.step()\n",
    "\n",
    "  if e % 4 == 3:\n",
    "    train_predictions = get_labels(model, x_train)\n",
    "    test_predictions = get_labels(model, x_test)\n",
    "    train_error = classification_error(y_train, train_predictions)\n",
    "    test_error = classification_error(y_test, test_predictions)\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}, train error: {train_error:.4f}, test error: {test_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = get_labels(model, x_train)\n",
    "test_predictions = get_labels(model, x_test)\n",
    "\n",
    "print(\"train error:\", f\"{classification_error(y_train, train_predictions):.4f}\")\n",
    "print(\"test error\", f\"{classification_error(y_test, test_predictions):.4f}\")\n",
    "\n",
    "display_confusion_matrix(y_train, train_predictions, display_labels=LFWUtils.LABELS)\n",
    "display_confusion_matrix(y_test, test_predictions, display_labels=LFWUtils.LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The result is mostly the same.\n",
    "\n",
    "We did add layers, but the network didn't need any extra neurons to do well on the training data.\n",
    "\n",
    "It needs help with the testing data, or, another way to say this is: it needs help generalizing without memorizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make It Harder\n",
    "\n",
    "Neural network models can seem simple to explain in a general sense: they're long and wide computation graphs made up of simple operations that have been tuned to achieve a specific task. Once they're training, or trained, their details and specificities are a little less easy to describe. It's hard to know exactly what each neuron is doing, and what part of the computation they are responsible for. We can train the same network, with the same parameters, using the same input data, and end up with wildly different results.\n",
    "\n",
    "This is one reason why it's hard to debug a network when it doesn't seem to be learning properly, or when it starts to overfit and memorize the training data. Which neurons do we tune ?\n",
    "\n",
    "One common situation that can lead to overfitting is when a network ends up with parameters that make it perform well on the training data without really activating all of its neurons. This is usually what is happening if adding layers to a network doesn't improve its performance.\n",
    "\n",
    "One set of strategies for improving neural network training in these cases involves making the training process harder than it has to be. It's like we're challenging the neural network to learn more than it has, so that later it has an easier time with the regular data.\n",
    "\n",
    "#### Dropout\n",
    "\n",
    "One simple technique to achieve this is to add `Dropout` layers to our network. A `Dropout` layer is a layer of neurons that don't perform any mathematical operation, but are selectively dropped out of the network randomly during training. This has the effect of randomly changing the network's architecture during training and preventing the network from becoming too reliant on specific neurons. Instead, it encourages the network to learn more robust features by activating more diverse sets of neurons.\n",
    "\n",
    "<img src=\"./imgs/dropout.jpg\" width=\"800px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Normalization\n",
    "\n",
    "Another technique that is used to keep our neural networks from memorizing data has to do with the range of the values that get passed between its inner layers.\n",
    "\n",
    "Input data coming into the network is most likely normalized, but after the first layer, the network weights might really change the distribution of the data as it flows through the network. Moreover, individual batches with different input value distributions can bias the network towards certain goals.\n",
    "\n",
    "<img src=\"./imgs/norm_activation.jpg\" width=\"720px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalization\n",
    "\n",
    "One way to handle these situations is to normalize the data as it passes through the network. Batch Normalization is the process of normalizing the activations of our network by using the mean and standard deviation of an activation neuron across a batch. The result is that the activations between batches become more similar. Batch normalization is dependent on batch size, so it's not effective for small batches.\n",
    "\n",
    "<img src=\"./imgs/norm_batch.jpg\" width=\"720px\"/>\n",
    "\n",
    "#### Layer Normalization\n",
    "\n",
    "Another form of inner-network normalization can be added to make sure no individual layer overpowers the network with activation values that are too large or too small.\n",
    "\n",
    "Layer Normalization scales activations using the mean and standard deviation of all activations across a layer. It's effective for sequence models like RNNs and Transformers, and for scenarios with small batch sizes, and doesn't require a large batch to get a good estimate for mean and standard deviation. \n",
    "\n",
    "<img src=\"./imgs/norm_layer.jpg\" width=\"720px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  nn.Sequential(\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(x_train.shape[1], x_train.shape[1] // 2),\n",
    "  nn.LayerNorm(x_train.shape[1] // 2),\n",
    "  nn.Sigmoid(),\n",
    "  # nn.BatchNorm1d(x_train.shape[1] // 2),\n",
    "\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(x_train.shape[1] // 2, x_train.shape[1] // 4),\n",
    "  nn.LayerNorm(x_train.shape[1] // 4),\n",
    "  nn.Sigmoid(),\n",
    "  # nn.BatchNorm1d(x_train.shape[1] // 4),\n",
    "\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(x_train.shape[1] // 4, x_train.shape[1] // 16),\n",
    "  nn.LayerNorm(x_train.shape[1] // 16),\n",
    "  nn.Sigmoid(),\n",
    "  # nn.BatchNorm1d(x_train.shape[1] // 16),\n",
    "\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(x_train.shape[1] // 16, len(y_train.unique())),\n",
    ")\n",
    "\n",
    "learning_rate = 1e-1\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "out = model(x_train)\n",
    "\n",
    "print(\"Input shape:\", x_train.shape)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(\"Num Params:\", get_num_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(32):\n",
    "  optim.zero_grad()\n",
    "  labels_pred = model(x_train)\n",
    "  loss = loss_fn(labels_pred, y_train)\n",
    "  loss.backward()\n",
    "  optim.step()\n",
    "\n",
    "  if e % 4 == 3:\n",
    "    train_predictions = get_labels(model, x_train)\n",
    "    test_predictions = get_labels(model, x_test)\n",
    "    train_error = classification_error(y_train, train_predictions)\n",
    "    test_error = classification_error(y_test, test_predictions)\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}, train error: {train_error:.4f}, test error: {test_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The train and test eval function diverged, but both keep decreasing, so this might be ok.\n",
    "\n",
    "In the end, the network seems capable of learning for longer and while the classification error for the test dataset doesn't keep up with the error in the train dataset, it does perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = get_labels(model, x_train)\n",
    "test_predictions = get_labels(model, x_test)\n",
    "\n",
    "print(\"train error\", f\"{classification_error(y_train, train_predictions):.4f}\")\n",
    "print(\"test error\", f\"{classification_error(y_test, test_predictions):.4f}\")\n",
    "\n",
    "display_confusion_matrix(y_train, train_predictions, display_labels=LFWUtils.LABELS)\n",
    "display_confusion_matrix(y_test, test_predictions, display_labels=LFWUtils.LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Information\n",
    "\n",
    "Our fully-connected layers do ok for this dataset, but they are not very efficient.\n",
    "\n",
    "There are $2$ main problems with using this approach to extract information about images:\n",
    "\n",
    "### Every pixel is connected to every other pixel\n",
    "\n",
    "Consider the first layer after the input layer: every neuron gets information about every pixel. This means that the content at the top-left corner of our image is connected to the content at the bottom-right corner, which is inefficient. We probably don't need our network to consider the entire content of the image at once in order to make decisions. It jumbles the pixel order and just makes the process harder. We might be better off telling our network to consider groups of neighboring pixels, since it's most likely for visual features to come from pixels that are near each other. In other words, we want to extract and preserve some kind of relative _Locality_ from our pixels.\n",
    "\n",
    "### Not all Arnolds are the same\n",
    "\n",
    "Let's say our network learned how to classify an Arnold Schwarzenegger that's closer to the left side of the image. If it wants to detect Arnolds on the right side of the image, or towards the top, it has to learn how to activate neurons that are associated with those sections of the image. This is also inefficient because it has to relearn to detect the same thing again, just because it's somewhere else in the image.\n",
    "\n",
    "Again, what we would like to do is group neighboring pixels, and have the groups go through similar neurons, so that any kind of learning can be applied independent of where shapes are located in the image. The technical name for this property is _Translation Invariance_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions\n",
    "\n",
    "We could try to come up with our own architecture and write some code for a neural network that doesn't fully connect our pixels, but rather considers neighboring regions of our image in groups of neurons.\n",
    "\n",
    "But, luckily, some maths combined with intuition from old-school systems and feature engineering can help us here.\n",
    "\n",
    "There's a type of mathematical operation called a convolution that combines $2$ arbitrary functions into a new function that basically has information about all the possible combinations of inputs for the $2$ original functions.\n",
    "\n",
    "<!-- The math looks like this for the $1D$ continuous case:<br> -->\n",
    "<!-- $\\displaystyle (f * g)(\\tau) = \\int_{-\\infty}^{\\infty}{f(\\tau)\\ g(1 - \\tau)\\ d\\tau}$ -->\n",
    "\n",
    "The math looks like this for the $1D$ case:<br>\n",
    "$\\displaystyle (f * g)[n] = \\sum_{k=-K}^{K}{f[k]\\ \\ g[n - k]}$\n",
    "\n",
    "\n",
    "<br>And the $2D$ case:<br>\n",
    "$\\displaystyle (f * g)[n_x, n_y] = \\sum_{k_x=-K}^{K}{\\sum_{k_y=-K}^{K}{f[k_x, k_y]\\ \\ g[n_x - k_x, n_y - k_y]}}$\n",
    "\n",
    "For the practical, intuitive, definition of this operation when dealing with images, $f[\\ ]$ is an image tensor and $g[\\ ]$ can be different, but specific, combinations of numbers organized into $2D$ matrices called kernels.\n",
    "\n",
    "When we _convolve_ the image with the kernel, we calculate every possible overlap of our kernel with the image and, depending on the numbers we choose for the kernel, can extract different types of features from our pixels.\n",
    "\n",
    "<!-- <img src=\"./imgs/kernel_slide.jpg\" height=\"256px\"/> -->\n",
    "<img src=\"https://i.postimg.cc/wMzHtFnJ/kernel-slide.jpg\" height=\"256px\"/>\n",
    "\n",
    "[SOME ANIMATIONS](https://hannibunny.github.io/mlbook/neuralnetworks/convolutionDemos.html)\n",
    "\n",
    "\n",
    "Classic image processing kernels for sharpening an image and extracting edges:\n",
    "\n",
    "<!-- <img src=\"./imgs/image_kernels.jpg\" height=\"300px\"/> -->\n",
    "<img src=\"https://i.postimg.cc/fTKWdnzb/image-kernels.jpg\" height=\"300px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nice thing about these kernels is that they operate on neighboring pixels by default, so they already take into account the _locality_ of the features they're trying to detect.\n",
    "\n",
    "We can now set up a neural network that is a collection of $2D$ image kernels, and let our training algorithm learn parameters for these kernels based on the training data. We don't have to specify that we want an edge-detection kernel, or a curved-shape kernel, or a horizontal blur kernel... the network will learn the kernels that it needs.\n",
    "\n",
    "And, since the same kernel slides over an entire image during convolution, once the network learns to extract lines on the left side of the image, it also knows how to extract lines on the right side of the image, or on top, or anywhere else. The parameters to the kernel are the same, they just get applied to different neighborhoods of pixels.\n",
    "\n",
    "If we combine our bank of kernels with another operation to reduce the size of our image as it moves through the network, we can create a type of dynamic filtering that detects whether certain features are present on our image.\n",
    "\n",
    "<!-- <img src=\"./imgs/cnn_layers.jpg\" height=\"320px\"/> -->\n",
    "<img src=\"https://i.postimg.cc/rpdq7DSd/cnn-layers.jpg\" height=\"320px\"/>\n",
    "\n",
    "Then, after we have reduced our feature maps to a small-enough shape we ca use fully-connected layers to finalize our classification.\n",
    "\n",
    "<!-- <img src=\"./imgs/cnn_fc.jpg\" height=\"320px\"/> -->\n",
    "<img src=\"https://i.postimg.cc/XYYjcHqk/cnn-fc.jpg\" height=\"320px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Network, New Shape\n",
    "\n",
    "Right now our input samples are lists of pixels, organized in a `tensor` of shape $(1, 22100)$. In order to use the convolution layers we actually need the images in a $2D$ tensor of shape $(170, 130)$.\n",
    "\n",
    "CNNs are even more particular about channels: we not only have to be explicit about the number of channels on our images, but this info also has to be in the first dimension of our image tensors.\n",
    "\n",
    "So, instead of working with the original image shape where each pixel keeps the values for its channels together, like:<br>\n",
    "$H \\times W \\times C$ (`height` by `width` by `channel`)\n",
    "\n",
    "we now have to use a `tensor` made up of single-channel images: all the RED values, then all the GREEN values, then all the BLUE values... so a $3D$ `tensor` of shape: $C \\times H \\times W$. For our `LFW` images this is $(1, 170, 130)$.\n",
    "\n",
    "The [`permute()`](https://pytorch.org/docs/stable/generated/torch.permute.html) or [`movedim()`](https://pytorch.org/docs/stable/generated/torch.movedim.html#torch.movedim) functions can be used to reorganize the order of a `tensor`'s dimensions.\n",
    "\n",
    "In the end, our dataset should be in a `tensor` of shape:<br>\n",
    "$N \\times C \\times H \\times W$ (`samples`, `channels`, `height`, `width`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = LFWUtils.train_test_split(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're working with grayscale images in this exercise, but the code below should also work for `RGB` images by changing `n_channels` to $3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iw,ih = LFWUtils.IMAGE_SIZE\n",
    "n_channels = 1\n",
    "n_samples = len(train[\"pixels\"])\n",
    "\n",
    "x_train = Tensor(train[\"pixels\"]) # 445 x 22100\n",
    "x_train = x_train.reshape(n_samples, LFWUtils.IMAGE_HEIGHT, LFWUtils.IMAGE_WIDTH, n_channels) # 445 x 170 x 130 x 1\n",
    "x_train = x_train.permute(0,3,1,2) # 445 x 1 x 170 x 130\n",
    "\n",
    "# reshape 438 x 22100 into 438 x 1 x 170 x 130\n",
    "x_test = Tensor(test[\"pixels\"]).reshape(-1, LFWUtils.IMAGE_HEIGHT, LFWUtils.IMAGE_WIDTH, n_channels).movedim(-1, 1)\n",
    "\n",
    "y_train = Tensor(train[\"labels\"]).long()\n",
    "y_test = Tensor(test[\"labels\"]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset Samples\")\n",
    "print(\"\\tTrain:\", len(x_train))\n",
    "print(\"\\tTest:\", len(x_test))\n",
    "\n",
    "print(\"\\nDataset Shape:\")\n",
    "print(f\"\\tTrain:\", list(x_train.shape))\n",
    "print(f\"\\tTtest:\", list(x_test.shape))\n",
    "\n",
    "print(\"\\nSample Shape:\")\n",
    "print(f\"\\tTrain:\", list(x_train[0].shape))\n",
    "print(f\"\\tTest:\", list(x_test[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "img_t = x_train[idx].permute(1,2,0).reshape(-1, x_train.shape[1])\n",
    "display(make_image(img_t, width=x_train.shape[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CNN Model\n",
    "\n",
    "This is how we define a convolution layer:\n",
    "\n",
    "`nn.Conv2d(Cin, Cout, kernel_size)`\n",
    "\n",
    "Where `Cin` is the number of input channels, `Cout` is output channels, and `kernel_size` the width of our kernel.\n",
    "\n",
    "We should still normalize the computations by batch, but this time using the $2D$ version of `BatchNorm()`, and after activation we perform the `MaxPool` operation, which takes the largest value in a $2 \\times 2$ region of our activations and condenses them into denser representation of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_length = 70848\n",
    "\n",
    "model = nn.Sequential(\n",
    "  nn.Conv2d(1, 32, 7),\n",
    "  nn.BatchNorm2d(32),\n",
    "  nn.Sigmoid(),\n",
    "  nn.MaxPool2d(3),\n",
    "\n",
    "  nn.Flatten(),\n",
    "\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(linear_length, len(LFWUtils.LABELS)),\n",
    ")\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "out = model(x_train)\n",
    "\n",
    "print(\"Input shape:\", x_train.shape)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(\"Parameters:\", get_num_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(32):\n",
    "  optim.zero_grad()\n",
    "  labels_pred = model(x_train)\n",
    "  loss = loss_fn(labels_pred, y_train)\n",
    "  loss.backward()\n",
    "  optim.step()\n",
    "\n",
    "  if e % 4 == 3:\n",
    "    train_predictions = get_labels(model, x_train)\n",
    "    test_predictions = get_labels(model, x_test)\n",
    "    train_error = classification_error(y_train, train_predictions)\n",
    "    test_error = classification_error(y_test, test_predictions)\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}, train error: {train_error:.4f}, test error: {test_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "The CNN architecture is so stable that models can be made to be very deep, some with $100\\text{s}$ of layers.\n",
    "\n",
    "The internal layers of these models are so abstract and generic that once a model has been trained on millions of data samples (images), it learns and retains information not only about the images on the dataset, but any visual pattern that it learned in the process.\n",
    "\n",
    "It's not uncommon to use a previously trained model for a similar-but-different project, even if the images have nothing in common. Generic information about images can be transferred to new datasets and problem spaces.\n",
    "\n",
    "<!-- <img src=\"./imgs/resnet_activation_00.jpg\" height=\"300px\" /> -->\n",
    "<img src=\"https://i.postimg.cc/tR3twzmz/resnet-activation-00.jpg\" height=\"300px\" />\n",
    "\n",
    "<!-- <img src=\"./imgs/resnet_activation_01.jpg\" height=\"300px\" /> -->\n",
    "<img src=\"https://i.postimg.cc/hPKbB7kR/resnet-activation-01.jpg\" height=\"300px\" />\n",
    "\n",
    "<!-- <img src=\"./imgs/resnet_activation_02.jpg\" height=\"300px\" /> -->\n",
    "<img src=\"https://i.postimg.cc/15M053Sn/resnet-activation-03.jpg\" height=\"300px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Networks\n",
    "\n",
    "There are a couple of families of CNN networks that get used as the starting point for many different types of visual models (and also audio and text). One such architecture is [ResNet](https://arxiv.org/abs/1512.03385).\n",
    "\n",
    "ResNet comes in a few sizes/depths, and PyTorch has at least [5 pre-trained ResNet models](https://pytorch.org/hub/pytorch_vision_resnet/) that we can use.\n",
    "\n",
    "These PyTorch ResNet models were trained on the [ImageNet](https://image-net.org/download.php) dataset. This dataset has $1\\text{,}281\\text{,}167$ training images and classifies objects into $1\\text{,}000$ classes.\n",
    "\n",
    "We'll use the `ReNet34` model, which is not the largest, but will fit nicely into small GPUs.\n",
    "\n",
    "<!-- <img src=\"./imgs/resnet34_00.jpg\" width=\"900px\" /> -->\n",
    "<img src=\"https://i.postimg.cc/XNc8xdqy/resnet34-00.jpg\" width=\"900px\" />\n",
    "\n",
    "<!-- <img src=\"./imgs/resnet34_01.jpg\" width=\"900px\" /> -->\n",
    "<img src=\"https://i.postimg.cc/hP20Rn9D/resnet34-01.jpg\" width=\"900px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating ResNet\n",
    "\n",
    "Is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "display(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust inputs\n",
    "\n",
    "From https://pytorch.org/hub/pytorch_vision_resnet/:\n",
    "\n",
    "_All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]._\n",
    "\n",
    "We can use `PyTorch` transformation functions to achieve this, but this means that now we'll have some transformations that always have to happen and some that only happen in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_transforms = v2.Compose([\n",
    "  v2.ToDtype(torch.uint8),\n",
    "  v2.Resize(224),\n",
    "  v2.Grayscale(3),\n",
    "  v2.ToDtype(torch.float32, scale=True),\n",
    "  v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_res = res_transforms(x_train).to(\"cuda\")\n",
    "x_test_res = res_transforms(x_test).to(\"cuda\")\n",
    "\n",
    "print(\"Training dataset shape:\", x_train_res.shape)\n",
    "print(\"Image shape:\", x_train_res[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_res[0].min(), x_train_res[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(model.fc.in_features, len(LFWUtils.LABELS))\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "learning_rate = 5e-3\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "out = model(x_train_res[::3])\n",
    "\n",
    "print(\"Input shape:\", x_train_res.shape)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(\"Parameters:\", get_num_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.to(\"cuda\")\n",
    "y_test = y_test.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_step = 3\n",
    "\n",
    "for e in range(16):\n",
    "  model.train()\n",
    "  for batch_start in range(batch_step):\n",
    "    optim.zero_grad()\n",
    "    labels_pred = model(x_train_res[batch_start::batch_step])\n",
    "    loss = loss_fn(labels_pred, y_train[batch_start::batch_step])\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "  if e % 4 == 3:\n",
    "    train_predictions = get_labels(model, x_train_res)\n",
    "    test_predictions = get_labels(model, x_test_res)\n",
    "    train_error = classification_error(y_train.cpu(), train_predictions)\n",
    "    test_error = classification_error(y_test.cpu(), test_predictions)\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}, train error: {train_error:.4f}, test error: {test_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = get_labels(model, x_train_res)\n",
    "test_predictions = get_labels(model, x_test_res)\n",
    "train_error = classification_error(y_train.cpu(), train_predictions)\n",
    "test_error = classification_error(y_test.cpu(), test_predictions)\n",
    "print(f\"train error: {train_error:.4f}, test error: {test_error:.4f}\")\n",
    "\n",
    "display_confusion_matrix(y_train.cpu(), train_predictions, display_labels=LFWUtils.LABELS)\n",
    "display_confusion_matrix(y_test.cpu(), test_predictions, display_labels=LFWUtils.LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Layers\n",
    "\n",
    "That worked really well. The information learned by the `ResNet` network on 1 million images seems to transfer to our classification of faces and we can leverage its pattern-recognition layers to build a more accurate model in a short amount of time.\n",
    "\n",
    "Let's take a look at some of the filtered images in our `ResNet` model. We can do this with an untrained model, but it's better to look at one that has been recently trained.\n",
    "\n",
    "When we displayed the model layers above we saw that the model has $4$ main groups of convolution layers. The further down the model we go, the smaller the images are, and the more abstract the activation patterns will be.\n",
    "\n",
    "At the very last layer we might have $512$ _images_ that are only $4 \\times 4$ pixels, but light-up under very specific conditions, like: is there a bird in the image ? is there a face ?\n",
    "\n",
    "To see slightly larger images we'll look at some activations on layers $1$ and $2$.\n",
    "\n",
    "We'll use the `hook` mechanism from `PyTorch` to add some auxiliary logic to the layers we are interested in looking at. This allows us to run some extra code on the layers inputs and outputs every time it processes an image.\n",
    "\n",
    "Our `hook` function will just save the layers input and output tensors to external dictionaries that we cn visualize later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_in = {}\n",
    "activations_out = {}\n",
    "layer_kernels = {}\n",
    "\n",
    "def get_activation(name):\n",
    "  def hook(model, input, output):\n",
    "    if name not in layer_kernels:\n",
    "      layer_kernels[name] = model.weight.detach()\n",
    "    activations_in[name] = input[0].detach()\n",
    "    activations_out[name] = output.detach()\n",
    "  return hook\n",
    "\n",
    "model.conv1.register_forward_hook(get_activation('conv1'))\n",
    "model.layer1[1].conv2.register_forward_hook(get_activation('layer1.1.conv2'))\n",
    "model.layer1[2].conv2.register_forward_hook(get_activation('layer1.2.conv2'))\n",
    "model.layer2[0].conv2.register_forward_hook(get_activation('layer2.0.conv2'))\n",
    "model.layer4[0].conv2.register_forward_hook(get_activation('layer4.0.conv2'))\n",
    "model.layer4[2].conv2.register_forward_hook(get_activation('layer4.2.conv2'))\n",
    "model = model.to(mdevice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our `hook` in place we have to pass some data through the network so it saves the inputs and outputs for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  model(x_train_res[0:128].to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can display activations for specific images in the processed batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx = 0\n",
    "channel_idx = 0\n",
    "\n",
    "img_t = x_train[img_idx, channel_idx]\n",
    "\n",
    "display(make_image(img_t, width=img_t.shape[-1]))\n",
    "display_activation_grids(activations_out, img_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also visualize the kernel values in each layer.\n",
    "\n",
    "These are tiny $3 \\times 3$ or $7 \\times 7$ convolution kernels that get multiplied to filter the images.\n",
    "\n",
    "Other than the first one, where the kernels actually acted on 3-channel layers, the colors on the other kernels is artificial. They're the result of combining the kernels into groups of $3$, but in reality they are 64-channel kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_kernel_grids(layer_kernels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "9103",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
