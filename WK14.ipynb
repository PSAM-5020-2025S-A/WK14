{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 14\n",
    "\n",
    "More Neural Networks for images... and CNNs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/src/data_utils.py\n",
    "!wget -q https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/src/image_utils.py\n",
    "!wget -q https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/src/nn_utils.py\n",
    "!wget -q https://github.com/PSAM-5020-2025S-A/WK14/raw/main/WK14_utils.py\n",
    "\n",
    "!wget -qO- https://github.com/PSAM-5020-2025S-A/5020-utils/releases/latest/download/lfw.tar.gz | tar xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from data_utils import LFWUtils, classification_error, display_confusion_matrix\n",
    "from image_utils import make_image\n",
    "from nn_utils import get_labels, get_num_params\n",
    "\n",
    "from WK14_utils import display_activation_grids, display_kernel_grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTRO !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = LFWUtils.train_test_split(0.5)\n",
    "\n",
    "iw,ih = LFWUtils.IMAGE_SIZE\n",
    "nc = len(train[\"pixels\"][0][0]) if type(train[\"pixels\"][0][0]) == list else 1\n",
    "\n",
    "x_train = Tensor(train[\"pixels\"]).reshape(-1, ih, iw, nc).movedim(-1,1)\n",
    "y_train = Tensor(train[\"labels\"]).long()\n",
    "\n",
    "x_test = Tensor(test[\"pixels\"]).reshape(-1, ih, iw, nc).movedim(-1,1)\n",
    "y_test = Tensor(test[\"labels\"]).long()\n",
    "\n",
    "print(\"Dataset Samples\")\n",
    "print(\"\\tTrain:\", len(x_train))\n",
    "print(\"\\tTest:\", len(x_test))\n",
    "\n",
    "print(\"\\nDataset Shape:\", list(x_train.shape))\n",
    "print(\"\\nSample Shape:\", list(x_train[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model, Optimizer, Cost/Loss Function\n",
    "\n",
    "This is the model from last week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "The CNN architecture is so stable that models can be made to be very deep, some with $100\\text{s}$ of layers.\n",
    "\n",
    "The internal layers of these models are so abstract and generic that once a model has been trained on millions of data samples (images), it learns and retains information not only about the images on the dataset, but any visual pattern that it learned in the process.\n",
    "\n",
    "It's not uncommon to use a previously trained model for a similar-but-different project, even if the images have nothing in common. Generic information about images can be transferred to new datasets and problem spaces.\n",
    "\n",
    "<!-- <img src=\"./imgs/resnet_activation_00.jpg\" height=\"300px\" /> -->\n",
    "<img src=\"https://i.postimg.cc/tR3twzmz/resnet-activation-00.jpg\" height=\"300px\" />\n",
    "\n",
    "<!-- <img src=\"./imgs/resnet_activation_01.jpg\" height=\"300px\" /> -->\n",
    "<img src=\"https://i.postimg.cc/hPKbB7kR/resnet-activation-01.jpg\" height=\"300px\" />\n",
    "\n",
    "<!-- <img src=\"./imgs/resnet_activation_02.jpg\" height=\"300px\" /> -->\n",
    "<img src=\"https://i.postimg.cc/15M053Sn/resnet-activation-03.jpg\" height=\"300px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Networks\n",
    "\n",
    "There are a couple of families of CNN networks that get used as the starting point for many different types of visual models (and also audio and text). One such architecture is [ResNet](https://arxiv.org/abs/1512.03385).\n",
    "\n",
    "ResNet comes in a few sizes/depths, and PyTorch has at least [5 pre-trained ResNet models](https://pytorch.org/hub/pytorch_vision_resnet/) that we can use.\n",
    "\n",
    "These PyTorch ResNet models were trained on the [ImageNet](https://image-net.org/download.php) dataset. This dataset has $1\\text{,}281\\text{,}167$ training images and classifies objects into $1\\text{,}000$ classes.\n",
    "\n",
    "We'll use the `ReNet34` model, which is not the largest, but will fit nicely into small GPUs.\n",
    "\n",
    "<!-- <img src=\"./imgs/resnet34_00.jpg\" width=\"900px\" /> -->\n",
    "<img src=\"https://i.postimg.cc/XNc8xdqy/resnet34-00.jpg\" width=\"900px\" />\n",
    "\n",
    "<!-- <img src=\"./imgs/resnet34_01.jpg\" width=\"900px\" /> -->\n",
    "<img src=\"https://i.postimg.cc/hP20Rn9D/resnet34-01.jpg\" width=\"900px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating ResNet\n",
    "\n",
    "Is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "display(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust inputs\n",
    "\n",
    "From https://pytorch.org/hub/pytorch_vision_resnet/:\n",
    "\n",
    "_All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]._\n",
    "\n",
    "We can use `PyTorch` transformation functions to achieve this, but this means that now we'll have some transformations that always have to happen and some that only happen in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_transforms = v2.Compose([\n",
    "  v2.ToDtype(torch.uint8),\n",
    "  v2.Resize(224),\n",
    "  v2.Grayscale(3),\n",
    "  v2.ToDtype(torch.float32, scale=True),\n",
    "  v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_res = res_transforms(x_train)\n",
    "x_test_res = res_transforms(x_test)\n",
    "x_train_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_res[0].min(), x_train_res[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, len(LFWUtils.LABELS))\n",
    "model = model.to(mdevice)\n",
    "\n",
    "learning_rate = 5e-3\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "out = model(x_train_res[::3].to(\"cuda\"))\n",
    "\n",
    "print(\"Input shape:\", x_train_res.shape)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(\"Parameters:\", get_num_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_step = 3\n",
    "\n",
    "for e in range(16):\n",
    "  model.train()\n",
    "  for si in range(batch_step):\n",
    "    optim.zero_grad()\n",
    "    labels_pred = model(x_train_res[si::batch_step].to(\"cuda\"))\n",
    "    loss = loss_fn(labels_pred.to(\"cuda\"), y_train[si::batch_step].to(\"cuda\"))\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "  if e % 4 == 3:\n",
    "    train_predictions = get_labels(model, x_train_res.to(\"cuda\"))\n",
    "    test_predictions = get_labels(model, x_test_res.to(\"cuda\"))\n",
    "    train_error = classification_error(y_train, train_predictions)\n",
    "    test_error = classification_error(y_test, test_predictions)\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}, train error: {train_error:.4f}, test error: {test_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = get_labels(model, x_train_res.to(\"cuda\"))\n",
    "test_predictions = get_labels(model, x_test_res.to(\"cuda\"))\n",
    "train_error = classification_error(y_train, train_predictions)\n",
    "test_error = classification_error(y_test, test_predictions)\n",
    "print(f\"train error: {train_error:.4f}, test error: {test_error:.4f}\")\n",
    "\n",
    "display_confusion_matrix(y_train, train_predictions, display_labels=LFWUtils.LABELS)\n",
    "display_confusion_matrix(y_test, test_predictions, display_labels=LFWUtils.LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Layers\n",
    "\n",
    "That worked really well. The information learned by the `ResNet` network on 1 million images seems to transfer to our classification of faces and we can leverage its pattern-recognition layers to build a more accurate model in a short amount of time.\n",
    "\n",
    "Let's take a look at some of the filtered images in our `ResNet` model. We can do this with an untrained model, but it's better to look at one that has been recently trained.\n",
    "\n",
    "When we displayed the model layers above we saw that the model has $4$ main groups of convolution layers. The further down the model we go, the smaller the images are, and the more abstract the activation patterns will be.\n",
    "\n",
    "At the very last layer we might have $512$ _images_ that are only $4 \\times 4$ pixels, but light-up under very specific conditions, like: is there a bird in the image ? is there a face ?\n",
    "\n",
    "To see slightly larger images we'll look at some activations on layers $1$ and $2$.\n",
    "\n",
    "We'll use the `hook` mechanism from `PyTorch` to add some auxiliary logic to the layers we are interested in looking at. This allows us to run some extra code on the layers inputs and outputs every time it processes an image.\n",
    "\n",
    "Our `hook` function will just save the layers input and output tensors to external dictionaries that we cn visualize later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_in = {}\n",
    "activations_out = {}\n",
    "layer_kernels = {}\n",
    "\n",
    "def get_activation(name):\n",
    "  def hook(model, input, output):\n",
    "    if name not in layer_kernels:\n",
    "      layer_kernels[name] = model.weight.detach()\n",
    "    activations_in[name] = input[0].detach()\n",
    "    activations_out[name] = output.detach()\n",
    "  return hook\n",
    "\n",
    "model.conv1.register_forward_hook(get_activation('conv1'))\n",
    "model.layer1[1].conv2.register_forward_hook(get_activation('layer1.1.conv2'))\n",
    "model.layer1[2].conv2.register_forward_hook(get_activation('layer1.2.conv2'))\n",
    "model.layer2[0].conv2.register_forward_hook(get_activation('layer2.0.conv2'))\n",
    "model.layer4[0].conv2.register_forward_hook(get_activation('layer4.0.conv2'))\n",
    "model.layer4[2].conv2.register_forward_hook(get_activation('layer4.2.conv2'))\n",
    "model = model.to(mdevice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our `hook` in place we have to pass some data through the network so it saves the inputs and outputs for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  model(x_train_res[0:128].to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can display activations for specific images in the processed batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx = 0\n",
    "channel_idx = 0\n",
    "\n",
    "img_t = x_train[img_idx, channel_idx]\n",
    "\n",
    "display(make_image(img_t, width=img_t.shape[-1]))\n",
    "display_activation_grids(activations_out, img_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also visualize the kernel values in each layer.\n",
    "\n",
    "These are tiny $3 \\times 3$ or $7 \\times 7$ convolution kernels that get multiplied to filter the images.\n",
    "\n",
    "Other than the first one, where the kernels actually acted on 3-channel layers, the colors on the other kernels is artificial. They're the result of combining the kernels into groups of $3$, but in reality they are 64-channel kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_kernel_grids(layer_kernels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "9103",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
